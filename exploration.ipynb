{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19b89505-7681-41c0-a04c-0825db2b0e4f",
   "metadata": {},
   "source": [
    "# Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b51de48-ab5e-45f0-962b-9b0f0f1f9e1a",
   "metadata": {},
   "source": [
    "## Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5374ecf9-b535-4810-9d6a-e90f39c1ba17",
   "metadata": {},
   "source": [
    "1. [Overview](##1.-Overview)\n",
    "2. [Tools](##2.-Tools)\n",
    "3. [Exploration](##3.-Exploration)\n",
    "    - [Extract](###3.1-Extract)\n",
    "    - [Transform](###3.2-Transform)\n",
    "    - [Load](###3.3-Load)\n",
    "4. Evaluation Analysis\n",
    "    - [Confusion Matrix](###4.1-Confusion-Matrix)\n",
    "    - Precision\n",
    "    - Recall\n",
    "    - F1 Score\n",
    "    - Accuracy\n",
    "    - Specificity\n",
    "    - Autoevaluation\n",
    "5. [Building a Pipeline](##5.-Building-a-Pipeline)\n",
    "6. [Tests](##6.-Tests)\n",
    "7. [Conclusion](##7.-Conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6007c2c-42ed-457e-aeb1-b25f488efa73",
   "metadata": {},
   "source": [
    "## 1. Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990faafc-a1b6-4dc3-abf2-4dd44fd3f782",
   "metadata": {},
   "source": [
    "**Task:**\n",
    "\n",
    "The labellers have reviewed the output of a model, and now we would like to compare its performance against the ground truth. Attached are the results of the model run on 752 images, as well as the labels generated by the labellers. Each file lists the predictions of a single microscope image, with the filename denoting the image id. We would now like to compare the difference between the two datasets. Please write an efficient code base to output the F1 Score, Accuracy, Precision and Confusion Matrix of the dataset.\n",
    "\n",
    "**Inputs:**\n",
    "- Zip Folder\n",
    "- Folder titled \"predictions\"\n",
    "- JSON file describing predicted bounding boxes for a particular microscope image.\n",
    "- Folder titled \"Ground Truth\"\n",
    "- JSON file describing ground truth bounding boxes for a particular microscope image.\n",
    "\n",
    "**Outputs:**\n",
    "- F1 Score\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Confusion Matrix\n",
    "- Code used to generate the outputs\n",
    "\n",
    "**Submission**\n",
    "There are two ways to submit your code:\n",
    "1. Upload your repo to Github. Add ‘BioScout-Tom’ and ‘eltuna9’ as viewers to your repo.\n",
    "2. Use git bundle to create a single file of your repo, and reply to this email with the bundle attached.\n",
    "\n",
    "**Expectations:**\n",
    "- When using a Jupyter notebook, please use markdown to clearly document each section.\n",
    "- When using raw python, please make sure to commit frequently with clear commit messages, and to send through the bundled code.\n",
    "- Bonus: Extra points for any other metrics that you believe would be useful for measuring the performance of the model.\n",
    "\n",
    "**Goals:**\n",
    "In this question, we hope for you to demonstrate your knowledge of Python applied to a common ML data wrangling task, as well as modern software development practices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a26613-dab6-4cab-8330-9bf6351b7884",
   "metadata": {},
   "source": [
    "## 2. Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168337de-2278-475b-85b3-e9a02e03d031",
   "metadata": {},
   "source": [
    "To get started, we'll begin by importing a few packages that will help us carry out the tasks above. The two packages not in the standard library are:\n",
    "- `pandas`\n",
    "- `scikit-learn`\n",
    "- `ibis`\n",
    "- `dvc`\n",
    "- `OmegaConf`\n",
    "- `ruff`\n",
    "- `mypy`\n",
    "- `pandas-stubs`\n",
    "\n",
    "The following packages are used in the app version of this project only.\n",
    "- `streamlit`\n",
    "- `graphviz`\n",
    "- `seaborn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71446146-5b6b-4fa3-ad76-cd87e4deb53e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from pprint import pprint\n",
    "from glob import glob\n",
    "from os.path import join\n",
    "from random import choice\n",
    "from sklearn import metrics as m\n",
    "import ibis\n",
    "from omegaconf import OmegaConf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa91d708-f8d2-4872-80cb-6997190d0da2",
   "metadata": {},
   "source": [
    "## 3. Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c39cb47-5170-446b-bdf6-fbdb076551e4",
   "metadata": {},
   "source": [
    "Let's begin by grabbing all of the files in each directory and evaluating a random sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bb7dd2-d06a-48ea-8478-c88e5b9a452d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "actuals = glob(join(\"data\", \"raw\", \"ground_truth\", \"*.json\"))\n",
    "predicted = glob(join(\"data\", \"raw\", \"predictions\", \"*.json\"))\n",
    "predicted[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4964cb1-b36c-4686-b545-8d07d633cd95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "random_sample = choice(actuals)\n",
    "random_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19365be-35bd-42c0-a193-95f8833ca26c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(random_sample, \"r\") as sample:\n",
    "    data = json.load(sample)\n",
    "pprint(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3beaaa00-5b73-4623-bf9b-d35021008fe6",
   "metadata": {},
   "source": [
    "A much nicer way to look at this file would be through the [JSON Crack](https://jsoncrack.com/editor) editor, which would give us the following image.\n",
    "\n",
    "![jsonimage](images/jsonstruc.svg)\n",
    "\n",
    "\n",
    "Let's look at the prediction for the same image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a47415f-b377-406b-bcfe-f1fd1641a464",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# first we'll get the same file name and connect it to its respective directory\n",
    "random_prediction = join(\"data\", \"raw\", \"predictions\", random_sample.split(\"/\")[-1])\n",
    "\n",
    "with open(random_prediction, \"r\") as sample:\n",
    "    data = json.load(sample)\n",
    "pprint(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a5e355-1758-43a9-8529-2feef8c8d308",
   "metadata": {},
   "source": [
    "If we go to the [`schema_ref` url](https://darwin-public.s3.eu-west-1.amazonaws.com/darwin_json_2_0.schema.json) that comes up in the JSON output above, we can find more information about the files we are dealing with. Nonetheless, while the classification task at hand is one with bounding boxes rather than a **\"yes\"**, there are infectious spores in this sample, or **\"no\"**, there aren't any, we can still treat it as a binary classification task by using the absence of, or the lack thereof, the annotated boxes.\n",
    "\n",
    "Let's see how many samples lack an annotation for one or more bounding boxex from the ground truth sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3bcd05-15be-4d7e-b1cc-b728927a4b52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "box = 0\n",
    "no_box = 0\n",
    "not_sure = 0\n",
    "good_plants = []\n",
    "\n",
    "for file in actuals:\n",
    "    with open(file, \"r\") as s:\n",
    "        ac = json.load(s)\n",
    "        if ac[\"annotations\"]:\n",
    "            box += 1\n",
    "        elif ac[\"annotations\"] == []:\n",
    "            no_box += 1\n",
    "            good_plants.append(file)\n",
    "        else:\n",
    "            not_sure += 1\n",
    "            good_plants.append(file)\n",
    "print(f\"The ground truth sample has {box} images with at least one instance of a decease, {no_box} are healthy, and {not_sure} are unclear!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30664a98-6908-458e-ab9f-ce3b3a4d2c80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(choice(good_plants), \"r\") as s:\n",
    "    pprint(json.load(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6a66a0-0091-484b-a467-e52d084e820e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "for file in good_plants:\n",
    "    with open(file, \"r\") as s:\n",
    "        pprint(json.load(s)[\"annotations\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35a512d-2d98-4f89-9b23-e1d9de6b0290",
   "metadata": {},
   "source": [
    "Now that we know a bit about the files we're dealing with, let's create a few functions to programmatically extract, transform, load, and evaluate all samples available as well as future ones.\n",
    "\n",
    "Since we'll need to save files here and there throughout the steps we'll take in this notebook, we want to start with a straightforward load function that takes in a dataframe and saves it as a parquet file.\n",
    "\n",
    "Here, we take advantage of the ipython magic command `%%writefile`, which allows turn into a script everything contained in a cell. We will use this command a few more times throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b235dfe-c9e3-4425-8601-e167d6f4f87a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile src/load.py\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "def save_data(data, path_out, file_name):\n",
    "    path_out = Path(path_out)\n",
    "    if not path_out.exists(): path_out.mkdir(parents=True)\n",
    "    data.to_parquet(path_out.joinpath(file_name))\n",
    "    print(f\"Successfully loaded the {file_name} table!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb7e678-87a6-4c1c-9b02-c63ce6c4bf71",
   "metadata": {},
   "source": [
    "### 3.1 Extract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39632e0-0e77-4440-b852-f30653bee98f",
   "metadata": {},
   "source": [
    "Next, we'll need two functions:\n",
    "- One that collects all JSON files inside a directory\n",
    "- Another that\n",
    "    1. reads each of these files\n",
    "    2. extracts the the `name` and `prediction` from it\n",
    "    3. creates a dataframe for each sample and puts them in a list\n",
    "    4. creates one dataframe for all samples\n",
    "\n",
    "While there are quite a few pieces that could be separated in the last function, the goal is a single one, to create a dataframe the ground truth and the predicted data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13007f2b-2212-43b3-b4e3-22cfa3d97fab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_files(directory):\n",
    "    return glob(join(directory, \"*.json\"))\n",
    "\n",
    "\n",
    "def read_files(data_files: str) -> pd.DataFrame:\n",
    "        \n",
    "    dfs_list = []\n",
    "    \n",
    "    for file in data_files:    \n",
    "        with open(file, \"r\") as sample:\n",
    "            data = json.load(sample)\n",
    "\n",
    "        item = data[\"item\"]['name']\n",
    "\n",
    "        if data[\"annotations\"]:\n",
    "            anno_name = data[\"annotations\"][0][\"name\"]\n",
    "        else:\n",
    "            anno_name = \"Undetected\"\n",
    "        \n",
    "        df = pd.DataFrame(data=[[item, anno_name]], columns=[\"item_id\", \"class\"])\n",
    "        dfs_list.append(df)\n",
    "    \n",
    "    return pd.concat(dfs_list, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a6f300-74e2-41c9-95af-aa4369ff3168",
   "metadata": {},
   "source": [
    "Let's make sure our functions work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba77c265-aec9-41c9-bb93-4059f3fe4b01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "actuals = get_files(\"data/raw/ground_truth/\")\n",
    "actuals[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adc8155-4c03-4d10-ac0f-974d5274c162",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "read_files(actuals).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59dbc114-6ccc-4d5f-b0f0-ebdee28ac62b",
   "metadata": {},
   "source": [
    "Excellent! We'll save the function in our src directory and start preparing a package in case we come up with new functions for extracting data, or in case we want to update our current ones.\n",
    "\n",
    "In this file called `extract.py`, we'll add one more function called `load_table` since we'll need to load data in subsequent steps. In addition, we'll go ahead and call the functions in the order above and save the files into a directory called `interim`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca897f64-eed3-43e8-8295-6fb4a6f4a32f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile src/extract.py\n",
    "\n",
    "import pandas as pd\n",
    "from os.path import join\n",
    "from glob import glob\n",
    "from load import save_data\n",
    "import json\n",
    "\n",
    "def get_files(directory):\n",
    "    return glob(join(directory, \"*.json\"))\n",
    "\n",
    "\n",
    "def read_files(data_files: str) -> pd.DataFrame:\n",
    "        \n",
    "    dfs_list = []\n",
    "    \n",
    "    for file in data_files:    \n",
    "        with open(file, \"r\") as sample:\n",
    "            data = json.load(sample)\n",
    "\n",
    "        item = data[\"item\"]['name']\n",
    "\n",
    "        if data[\"annotations\"]:\n",
    "            anno_name = data[\"annotations\"][0][\"name\"]\n",
    "        else:\n",
    "            anno_name = \"Undetected\"\n",
    "        \n",
    "        df = pd.DataFrame(data=[[item, anno_name]], columns=[\"item_id\", \"class\"])\n",
    "        dfs_list.append(df)\n",
    "    \n",
    "    return pd.concat(dfs_list, axis=0)\n",
    "\n",
    "def load_table(data_path, file_name):\n",
    "    return pd.read_parquet(join(data_path, file_name))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    actuals = get_files(\"data/raw/ground_truth/\")\n",
    "    predictions = get_files(\"data/raw/predictions/\")\n",
    "    df_truth = read_files(actuals)\n",
    "    df_preds = read_files(predictions)\n",
    "    save_data(df_truth, join(\"data\", \"interim\"), \"actuals_table.parquet\")\n",
    "    save_data(df_preds, join(\"data\", \"interim\"), \"predicted_table.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03886d1f-90da-45c3-b556-6f629707675d",
   "metadata": {},
   "source": [
    "Let's test our script to make sure it works well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d69b3f-8936-4849-a5d4-fd24cb8b83b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python src/extract.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fb9119-5d43-4dc5-8412-e5dff3c85309",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_truth = pd.read_parquet(join(\"data\", \"interim\", \"actuals_table.parquet\"))\n",
    "df_truth.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda25493-5dfd-4697-98a8-ef75305ed619",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_preds = pd.read_parquet(join(\"data\", \"interim\", \"predicted_table.parquet\"))\n",
    "df_preds.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a25d1e-ad22-488a-9133-7b1f52a6d77a",
   "metadata": {},
   "source": [
    "### 3.2 Transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae448cf-9f86-4a8a-b5e4-928490cfd28e",
   "metadata": {},
   "source": [
    "The transform stage for this project will be very straightforward as what would be most helpful here is to combine both dataset to have the ground truth labes and the predicted ones, in two adjacent columns.\n",
    "\n",
    "We will create one function for this but note that this part of the process could be much more involved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f1e7b9-2e9f-402c-9e07-c1be64f1238c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def merge_truth_preds(df1, df2, **kwargs):\n",
    "    return pd.merge(left=df1, right=df2, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcc094f-cd8f-4323-9ed5-8d92f3ea2c97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = merge_truth_preds(df_truth, df_preds, left_on=\"item_id\", right_on=\"item_id\", suffixes=(\"_truth\", \"_pred\"))\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96d61c3-2575-43b9-a891-6cb077e30359",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.class_truth.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5c41ca-66d7-4d17-9238-3da045c31b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.class_pred.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6610fb07-6c4e-4d4b-b416-41f76306eeb1",
   "metadata": {},
   "source": [
    "Time to create a script to automate the process. We'll follow the same formula as before for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5107274-a736-43d4-a088-3c248057c72b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile src/transform.py\n",
    "\n",
    "import pandas as pd\n",
    "from os.path import join\n",
    "from extract import load_table\n",
    "from load import save_data\n",
    "\n",
    "def merge_truth_preds(df1, df2, **kwargs):\n",
    "    return pd.merge(left=df1, right=df2, **kwargs)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df1 = load_table(join(\"data\", \"interim\"), \"actuals_table.parquet\")\n",
    "    df2 = load_table(join(\"data\", \"interim\"), \"predicted_table.parquet\")\n",
    "    df_combined = merge_truth_preds(df1, df2, left_on=\"item_id\", right_on=\"item_id\", suffixes=(\"_truth\", \"_pred\"))\n",
    "    save_data(df_combined, join(\"data\", \"processed\"), \"combined_table.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcdd277-29ab-4c49-9f76-cb1f83a02121",
   "metadata": {},
   "source": [
    "Let's test our script to make sure everything is working properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef53f1e1-520b-4332-a11b-f095efa77d97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python src/transform.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f441a05-5aac-40f3-a3db-a346f1de3667",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_merged = pd.read_parquet(join(\"data\", \"processed\", \"combined_table.parquet\"))\n",
    "df_merged.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7aafe90-e4f6-437e-b36f-e96f612e2b01",
   "metadata": {},
   "source": [
    "### 3.3 Load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67b7a3b-6228-4a4b-9ba1-c7d5f94246c2",
   "metadata": {},
   "source": [
    "Note that more often than not data will be saved into a data warehouse or a data lake so that everyone in the team can access the files. With this in mind, let's update our load folder and mimic a data warehouse using DuckDB and Ibis. The former is a super fast in-memory database, and the latter is synthactic sugar for communicating with different databases.\n",
    "\n",
    "Our `create_db` function will create a DuckDB database and a table to store our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131cb5eb-3437-43d2-8f9d-ea99fd32e343",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile src/load.py\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import ibis\n",
    "from os.path import join\n",
    "\n",
    "def create_db(path_in, path_out, file_name, table_name):\n",
    "    path = Path(path_out)\n",
    "    conn = ibis.duckdb.connect(path.joinpath(file_name))\n",
    "    conn.register(path_in, table_name=table_name)\n",
    "    print(f\"Successfully loaded the {table_name} table!\")\n",
    "\n",
    "def save_data(data, path_out, file_name):\n",
    "    path_out = Path(path_out)\n",
    "    if not path_out.exists(): path_out.mkdir(parents=True)\n",
    "    data.to_parquet(path_out.joinpath(file_name))\n",
    "    print(f\"Successfully loaded the {file_name} table!\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    create_db(\n",
    "        path_in=join(\"data\", \"processed\", \"combined_table.parquet\"),\n",
    "        path_out=join(\"data\", \"dwarehouse\"),\n",
    "        file_name=\"db_analytics.ddb\",\n",
    "        table_name=\"truth_preds_challenge\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399271c1-1e5d-44a7-89dc-fc8f214aa4e0",
   "metadata": {},
   "source": [
    "Let's test it to make sure it works well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac625bc-5263-4436-97e8-a5ff37505ee3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python src/load.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e53c08-7f31-4c1f-adf8-36a92176693a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ibis\n",
    "con = ibis.duckdb.connect(join(\"data\", \"dwarehouse\", \"db_analytics.ddb\"))  # in-memory database\n",
    "con.list_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a235b4f8-e519-4ff9-91fc-5a0b4f36cb70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_preds = con.table(\"truth_preds_challenge\")\n",
    "data_preds.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f359828-3916-40fc-bfee-9e78019ca247",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_preds.to_pandas().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467cdcf2-250f-44aa-b5ca-b9af84a3644e",
   "metadata": {},
   "source": [
    "Now that we have processed the data, let's get started answering the questions for the challenge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16df134a-164c-4503-874b-031e0f688e99",
   "metadata": {},
   "source": [
    "## 4. Evaluation Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7d8fa7-c591-4465-be16-1d6907a8722b",
   "metadata": {},
   "source": [
    "### 4.1 Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02bd901-da25-460e-b8b9-55e50c7756a3",
   "metadata": {},
   "source": [
    "The fastest way to get started analyzing the results coming out of a classification model is via a confusion matrix (CM). CMs provide us with a $2x2$ (or bigger) matrix where the columns are represented by the actual labels, and the rows by the predictions. Each element in the table represents the intersection of the two.\n",
    "\n",
    "Here's a better way to visualize it. Imagine we are examining the predictions of a classification model sports cars.\n",
    "\n",
    "|       | Actual Sports Car | Not a Sports Car|\n",
    "|---|---|---|\n",
    "| Predicted a Sports Car  | 420 |  80 |\n",
    "| Predicted Not a Sports Car | 64 |  100 |\n",
    "\n",
    "The way we evaluate these regions is as follows.\n",
    "- `Actual Sports Car` and `Predicted a Sports Car` are considered `True Positives` since our model did a good jobs at predicting that a Ferrari is a sports car.\n",
    "- `Actual Sports Car` and `Predicted Not a Sports Car` are considered `False Negatives` since our model mistakenly said that a Ferrari was the same as a Honda CRV, not a sports car.\n",
    "- `Not a Sports Car` and `Predicted a Sports Car` are considered `False Positives` since our model predicted that Nissan Pathfinder was like a ferrari, a sports car.\n",
    "- `Not a Sports Car` and `Predicted Not a Sports Car` are considered `True Negatives` since our model did a good jobs at predicting that Honda CRVs are not sports cars.\n",
    "\n",
    "With our knowledge of confusion matrices, let's examine our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72996ad-89d5-4064-be59-ec38d3ed58ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mtx = m.confusion_matrix(df_merged.class_truth, df_merged.class_pred)\n",
    "mtx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dadaaa9-e210-4496-8430-f7535b839f2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conf = m.ConfusionMatrixDisplay(mtx)\n",
    "conf.plot(colorbar=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159c7f9b-34ba-4f32-a810-fed3c10ef707",
   "metadata": {},
   "source": [
    "As you can see from the image above, our model got\n",
    "- 688 True Positives\n",
    "- 0 True Negatives\n",
    "- 45 False Negatives\n",
    "- 19 False Positives\n",
    "\n",
    "This means that, while our model did well detecting images with spores carryng a decease, it still got confused with a few images. This could potentially result in having a farmer spray some unnecessary pesticide in its farm or doing nothing at all while it should be taking action.\n",
    "\n",
    "Let's examine a few other measures that help us understand the performance of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d94a5f-6841-4801-8c36-5ebd71b34d36",
   "metadata": {},
   "source": [
    "### 4.2 Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f411c7-6475-4ee0-aa86-beda943af2e7",
   "metadata": {},
   "source": [
    "**What is Precision?**\n",
    "> Precision measures the percentage of samples that are correctly identified as positive out of all samples that the model identified as positive. In other words, it measures the proportion of true positives among all the samples that the model classified as positive.\n",
    "\n",
    "$precision = \\frac{TP}{TP + FP}$\n",
    "\n",
    "Given the definition above, a high precision means that the model is good at avoiding false positives. Let's evaluate the precision of ours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83c60a6-4ffa-49c7-bdfa-14c737aed4b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "round(\n",
    "    m.precision_score(\n",
    "        df_merged.class_truth, \n",
    "        df_merged.class_pred, \n",
    "        pos_label='Alternaria spp.'\n",
    "    ) * 100,\n",
    "    2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb440a2-023d-489b-acfe-32264cace150",
   "metadata": {},
   "source": [
    "With a precision score of 93.86%, our models does very well at correctly classifying spores that carry out deceases while avoiding misclassifying those which don't have it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129f067c-cf73-4ed7-8c66-d8ad1ae72517",
   "metadata": {},
   "source": [
    "### 4.3 Recall/Sensitivity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acbd7de-639a-42df-9afa-1a408b019070",
   "metadata": {},
   "source": [
    "**What is Recall or Sencitivity?**\n",
    "> Recall measures the percentage of all positive items that are correctly identified by the model. In other words, it measures the proportion of true positives that the model correctly identified among all the samples that are actually positive.\n",
    "\n",
    "This means that if a model has a high recall it will be good at avoiding false negatives.\n",
    "\n",
    "$recall = \\frac{TP}{TP + FN}$\n",
    "\n",
    "Let's evaluate the recall of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda73b22-3711-49a0-8bcd-52146e6472a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "m.recall_score(df_merged.class_truth, df_merged.class_pred, pos_label='Alternaria spp.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65cf1a8-bd60-47ee-a6a7-e2496b1c087e",
   "metadata": {},
   "source": [
    "### 4.4 F1 Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b109daf4-22e1-416e-bb0b-ec0277b4e3a7",
   "metadata": {},
   "source": [
    "**What is an F1 Score?**\n",
    "\n",
    "> The F1 score is a measure of a model's accuracy that combines the precision and recall metrics into a single score. It takes into account both false positives and false negatives, making it a useful metric for evaluating models that deal with imbalanced classes.\n",
    "\n",
    "- $tp$ - True Positives\n",
    "- $tn$ - True Negatives\n",
    "- $fp$ - False Positives\n",
    "- $fn$ - False Negatives\n",
    "\n",
    "**F1 Formula**\n",
    "\n",
    "$f1 = 2\\frac{ (precision)(recall)}{precision + recall} = \\frac{2 tp}{2tp + fp + fn}$\n",
    "\n",
    "Let's get the F1 Score for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b1d145-6064-405b-b635-23790db7ed0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "round(\n",
    "    m.f1_score(\n",
    "        df_merged.class_truth, \n",
    "        df_merged.class_pred, \n",
    "        pos_label='Alternaria spp.'\n",
    "    ) * 100,\n",
    "    2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cc7f8d-cf93-433f-a066-e4cd4c0d7d15",
   "metadata": {},
   "source": [
    "As with precision and recall, the higher the score the better. In particular, a higher score means that our model can perform well on imbalanced data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1405ce-5ad2-4feb-92c3-8bb92cc37993",
   "metadata": {},
   "source": [
    "### 4.5 Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40dc0f24-1d69-4cf3-9e92-489a3f015349",
   "metadata": {},
   "source": [
    "**What is Accuracy?**\n",
    "> Accuracy is a metric used to evaluate how well a classification model is able to correctly predict the class label of unseen data points, and it is defined as the ratio of the number of correct predictions to the total number of predictions. \n",
    "\n",
    "While accuracy can be a very useful metric, it mostly shines with balanced datasets. Hence why we used more appropriate metrics such as precision, recall, or F1 score above.\n",
    "\n",
    "Nonetheless, let's get the precision for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d57c63-194f-43c5-8a99-00aba20ee142",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "round(\n",
    "    m.accuracy_score(\n",
    "        df_merged.class_truth, \n",
    "        df_merged.class_pred\n",
    "    ) * 100,\n",
    "    2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf09930-757f-465d-9c22-179ff22b8abc",
   "metadata": {},
   "source": [
    "### 4.6 Specificity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e0b1be-e663-4eea-b876-e6059931cd7e",
   "metadata": {},
   "source": [
    "One last metric we could have calculated is Specificity.\n",
    "\n",
    "**What is Specificity?**\n",
    "> Specificity is a metric used to evaluate the ability of a classification model to correctly predict the negative class, and it is represented as the proportion of true negative predictions over the total number of actual negatives.\n",
    "\n",
    "The above definition tells us that this metric is particularly useful in cases where the negative class is more important, such as in medical diagnosis, where correctly identifying healthy patients is critical. The problem with our analysis is that our **True Negatives** are 0, therefore, our specitivity will be $0$.\n",
    "\n",
    "Similar to accuracy, specificity may not be a sufficient metric in cases of class imbalance, but it is still useful for understanding the performance of our model.\n",
    "\n",
    "Even though we know it is $0$, let's use the formula to calculate it.\n",
    "\n",
    "$specificity = \\frac{tn}{tn + fp}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd46e257-b7d2-4963-b2ee-69bceb0e77be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mtx[1][1] / (mtx[1][1] + mtx[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c22ae12-3bba-495c-958f-59da6451e7a5",
   "metadata": {},
   "source": [
    "### 4.7 Autoevaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d447a9e-6412-49ea-b5c8-1097a485aeed",
   "metadata": {},
   "source": [
    "Let's start by creating functions for our metrics and finalize this section by turning these into a script as we have done before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76406162-98c8-4d35-8d30-120c68bceab7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_metrics(df, y_truth, y_pred, label):\n",
    "    return dict(\n",
    "        precision=round(m.precision_score(df[y_truth], df[y_pred], pos_label=label) * 100, 2),\n",
    "        recall=round(m.recall_score(df[y_truth], df[y_pred], pos_label=label) * 100, 2),\n",
    "        f1_score=round(m.f1_score(df[y_truth], df[y_pred], pos_label=label) * 100, 2),\n",
    "        accuracy=round(m.accuracy_score(df[y_truth], df[y_pred]) * 100, 2)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d8a3d5-50cb-42a9-befa-5c695c863621",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "metrics = get_metrics(df_merged, \"class_truth\", \"class_pred\", 'Alternaria spp.')\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b36a8d-d678-4f03-873c-3bf5ffb9bc8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def confused_mtx(df, y_truth, y_pred):\n",
    "    return m.confusion_matrix(df[y_truth], df[y_pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fce89e-46f8-45de-904d-98c36a443438",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conf_mtx = confused_mtx(df_merged, \"class_truth\", \"class_pred\")\n",
    "conf_mtx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d924973-76bf-4ef7-960b-6dafc54fda9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_config(metrics, mtx, path=None, file_name=None):\n",
    "    conf = OmegaConf.create({\n",
    "        \"facts\": {\n",
    "            \"sample_size\": int(sum(sum(mtx))),\n",
    "            \"healthy_preds\": int(mtx[0][0]),\n",
    "            \"wrong_preds\": int(mtx[1][0] - mtx[0][1])\n",
    "        },\n",
    "        \"metrics\": {k: float(v) for k, v in metrics.items()},\n",
    "        \"matrix\": {\n",
    "            \"true_pos\": int(mtx[0][0]),\n",
    "            \"true_neg\": int(mtx[1][1]),\n",
    "            \"false_pos\": int(mtx[0][1]),\n",
    "            \"false_neg\": int(mtx[1][0])\n",
    "        }\n",
    "    })\n",
    "    \n",
    "    print(OmegaConf.to_yaml(conf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca22958-a912-4dd7-92be-aa96f554735e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "generate_config(metrics, conf_mtx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0ffe55-7486-4edf-a7b1-72e3e3fc79ac",
   "metadata": {},
   "source": [
    "We can now put everything together and finalize our last script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08664f93-7f2a-42a9-a427-eb9a60fe75f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile src/evaluate.py\n",
    "\n",
    "import sklearn.metrics as m\n",
    "import pandas as pd\n",
    "from os.path import join\n",
    "from omegaconf import OmegaConf\n",
    "from extract import load_table\n",
    "\n",
    "def get_metrics(df, y_truth, y_pred, label):\n",
    "    return dict(\n",
    "        precision=round(m.precision_score(df[y_truth], df[y_pred], pos_label=label) * 100, 2),\n",
    "        recall=round(m.recall_score(df[y_truth], df[y_pred], pos_label=label) * 100, 2),\n",
    "        f1_score=round(m.f1_score(df[y_truth], df[y_pred], pos_label=label) * 100, 2),\n",
    "        accuracy=round(m.accuracy_score(df[y_truth], df[y_pred]) * 100, 2)\n",
    "    )\n",
    "\n",
    "def confused_mtx(df, y_truth, y_pred):\n",
    "    return m.confusion_matrix(df[y_truth], df[y_pred])\n",
    "\n",
    "def generate_config(metrics, mtx, path, file_name):\n",
    "    conf = OmegaConf.create({\n",
    "        \"facts\": {\n",
    "            \"sample_size\": int(sum(sum(mtx))),\n",
    "            \"healthy_preds\": int(mtx[0][0]),\n",
    "            \"wrong_preds\": int(mtx[1][0] - mtx[0][1])\n",
    "        },\n",
    "        \"metrics\": {k: float(v) for k, v in metrics.items()},\n",
    "        \"matrix\": {\n",
    "            \"true_pos\": int(mtx[0][0]),\n",
    "            \"true_neg\": int(mtx[1][1]),\n",
    "            \"false_pos\": int(mtx[0][1]),\n",
    "            \"false_neg\": int(mtx[1][0])\n",
    "        }\n",
    "    })\n",
    "    \n",
    "    OmegaConf.save(conf, join(path, file_name))\n",
    "    \n",
    "    print(f\"Config Successfully saved as {join(path, file_name)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = load_table(join(\"data\", \"processed\"), \"combined_table.parquet\")\n",
    "    metrics = get_metrics(df, \"class_truth\", \"class_pred\", 'Alternaria spp.')\n",
    "    conf_mtx = confused_mtx(df, \"class_truth\", \"class_pred\")\n",
    "    generate_config(metrics, conf_mtx, join(\"src\", \"configs\"), \"config.yml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575218d3-5041-4d4b-ab88-a44c760c36ed",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let's test it to make sure everything is working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efebf14-efa7-49e0-a7ab-3b3d904afacc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python src/evaluate.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5748f40e-3a05-4465-9472-f527666f44b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cat src/configs/config.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7057b7-cc99-4f09-b964-050ff389c6b2",
   "metadata": {},
   "source": [
    "## 5. Building a Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ed692f-d1c8-4c05-95f8-c3b59f50c4dd",
   "metadata": {},
   "source": [
    "We will be using dvc to create a reproducible pipeline, version, and cache artefacts.\n",
    "\n",
    "In order for us to create a pipeline via different stages, we need to have run the following commands first (no need to do this since you probably cloned this repo):\n",
    "- `dvc init`\n",
    "- `dvc remote add -d local_data_lake data`\n",
    "\n",
    "Now, we are ready to create our pipeline using `dvc stage add`, which will create a `dvc.yml` file which will track the different stages of our pipeline. Using the command `dvc repro` will run our pipeline and create a file called, `dvc.lock`, which will take care of versioning each component of it.\n",
    "\n",
    "If you want to run these files for the first time you can either remove them with \n",
    "```sh\n",
    "rm dvc.yml dvc.lock\n",
    "```\n",
    "or you can add `--force` in each of the steps, for example,\n",
    "```sh\n",
    "dvc stage add --force --name extract\n",
    "```\n",
    "or you can change some of the parameters like\n",
    "```sh\n",
    "dvc stage add --force --name extract \\\n",
    "    --deps data/raw/ground_truth/ \\\n",
    "    --deps data/raw/predictions/ \\\n",
    "    --outs your/data/dir/and/file.parquet \\\n",
    "    --outs your/data/dir/and/file.parquet \\\n",
    "    python src/extract.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da91501d-dbda-4a02-85e6-0110cebfcf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm dvc.lock dvc.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cbaaea-bc9d-4a57-a1c4-f64a33900130",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "dvc stage add --name extract \\\n",
    "    --deps data/raw/ground_truth/ \\\n",
    "    --deps data/raw/predictions/ \\\n",
    "    --outs data/interim/actuals_table.parquet \\\n",
    "    --outs data/interim/predicted_table.parquet \\\n",
    "    python src/extract.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7be562b-199b-4d45-98f6-c74db2ac8791",
   "metadata": {},
   "source": [
    "Let's make sure our pipeline works by calling `dvc repro`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98074ea0-181b-4f09-9bfe-9a82d0ad6523",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cat dvc.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056a026e-0253-4969-94e7-5fb546beff4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!dvc repro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32678ac6-cf67-465d-a70f-7c7e2f5006cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we don't need to add each part of the pipeline manually\n",
    "!dvc config core.autostage true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20faa914-2c28-47b7-874e-b02bc377b4b3",
   "metadata": {},
   "source": [
    "Let's add the last 3 sections of our pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f7a070-4000-4553-aa14-5836e768f309",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "dvc stage add --name transform \\\n",
    "    --deps data/interim/actuals_table.parquet \\\n",
    "    --deps data/interim/predicted_table.parquet \\\n",
    "    --outs data/processed/combined_table.parquet \\\n",
    "    python src/transform.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f996adfa-86e4-4d3f-a882-55614a4141bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "dvc stage add --name load \\\n",
    "    --deps data/processed/combined_table.parquet \\\n",
    "    --outs data/dwarehouse/db_analytics.ddb \\\n",
    "    python src/load.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08083a6d-78f8-48ae-a7c8-651594405900",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "dvc stage add --name evaluate \\\n",
    "    --deps data/processed/combined_table.parquet \\\n",
    "    python src/evaluate.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2141ec71-7206-4958-9143-a8e070b6e98d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!dvc repro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4f4e9d-fee1-4cc3-864d-7c632a635bae",
   "metadata": {},
   "source": [
    "Finally, if we want to have a look at the graph created by our pipeline, we can `dvc dag` for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2ccb57-7272-437f-8821-682028143e09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!dvc dag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1088b4-b1d0-446d-ab77-572a016fd665",
   "metadata": {},
   "source": [
    "## 6. Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b5814c-e331-4ef3-8f78-9f8ce0c16387",
   "metadata": {},
   "source": [
    "The fastest way to get started testing our code is with `ruff`, a blazingly fast Python linter. In addition, we could go back and add type annotations to our scripts so that we can test code correctness with `mypy`.\n",
    "\n",
    "It is important to note that we will use them here for example purposes only, and not to be included in a rigorous CI/CD pipeline (that's for another day 😎)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ebd422-2c5f-48f3-9a72-fd518893814e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ruff check ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d87b0e9-2427-4344-89d8-c7865561da1e",
   "metadata": {},
   "source": [
    "As you can see, `ruff` will alert us of any unused modules, long titles, and non-pythonic code found in our codebase, which becomes very useful when collaborating with others on large projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b3afdb-4889-46f8-a7c7-917477cbcd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mypy src/extract.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adde80bd-b395-4ce2-beb5-839baa6dbae0",
   "metadata": {},
   "source": [
    "Because we have not created a local dev environment for our package, mypy will let us know that the load module is an unknown one for it and, therefore, we need to add such implementation as stub package to PyPI (similar to `pandas-stubs`).\n",
    "\n",
    "There is also a type we need to fix, `List`, inside our `extract.py` scrip. We need to add `str` inside of it as `List[str]` so that mypy doesn't yell at us.\n",
    "\n",
    "That was a quit intro to a few quick and dirty tests we can run on our codebase, there are plenty more we could take advantage of, of course."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fc1499-57ce-4a3c-8261-eaa074e8e920",
   "metadata": {},
   "source": [
    "## 7. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704e67f1-c55e-43f0-bcda-92b779585c1b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ceb2d4-3f1f-4468-814a-0a734617cc04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd65893e-5fcf-4e52-8e4a-dc6d79b220f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
